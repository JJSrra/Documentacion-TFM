\chapter{Diseño experimental}

En este capítulo se abordarán todas las cuestiones relativas a la fase de experimentación del estudio, desde las primeras tomas de decisiones a la hora de formar grupos hasta las métricas elegidas para validar las soluciones. A lo largo de los siguientes párrafos se darán argumentos que defiendan las posturas que se han tenido que tomar y se dará una visión suficientemente detallada del experimento para que los resultados sean fácilmente interpretables.

El esquema en el que se ha basado este experimento es el que plantearon De Campos et al. en su publicación titulada \textit{``Managing uncertainty in group recommending processes''} \cite{umuai} que vio la luz en 2009. Además, más en particular, la creación de grupos ha sido bastante influenciada por lo planteado en esta publicación.

En términos generales, el experimento transcurrirá a lo largo de las siguientes fases:

\begin{enumerate}
	\item \textbf{Elección del conjunto de datos.} El primer paso necesario es seleccionar un conjunto de datos que se adecúe al objetivo del estudio; en este caso, que cuente con una buena cantidad de películas y valoraciones para poder llevar a cabo el estudio sobre sistemas de recomendación a grupos en este ámbito.
	\item \textbf{Creación de grupos.} Para llevar a cabo el experimento es necesario definir sobre qué grupos se va a trabajar y crearlos desde un primer momento, para así experimentar sobre los mismos grupos pero en diferentes métodos de recomendación. Se utilizarán dos estrategias distintas a la hora de crear grupos.
	\item \textbf{Guardado de información reutilizable de interés.} Para el funcionamiento de algunos de los métodos, o como herramienta útil en algún procesamiento interno, se guardarán algunos datos estáticos tras haberlos calculado una única vez, y no tener que emplear tiempo de ejecución innecesario cada vez que se repita el experimento. Entre estos datos se encuentran los grupos formados, pero también la matriz de correlación de Pearson o las valoraciones de cada usuario.
	\item \textbf{Filtrado de películas evaluables por grupo.} Al haber una gran cantidad de películas, muchas de las que ocupan el conjunto de validación pueden no haber sido vistas por ninguno de los miembros del grupo, siendo en este caso absurdo hacerlas formar parte del experimento pues no existen datos reales con los que validar la recomendación. Más adelante se especificará más concretamente qué criterio se ha seguido para escoger las películas de cada grupo.
	\item \textbf{Diseño de la evaluación de los modelos.} Se debe diseñar con cautela el método de evaluación, obteniendo los valores reales con los que comparar los resultados de los distintos recomendadores y además escoger una adecuada métrica de evaluación. Puesto que en este experimento se cuenta con soluciones en formato de ranking, se debe buscar una métrica especial capaz de interpretar adecuadamente la bondad de una solución de este tipo.
	\item \textbf{Evaluación de los métodos del estudio.} Primero se ha de comprobar el algoritmo base, conocido como \textit{baseline}, a continuación se comprueba que la implementación del estado del arte funciona correctamente y por último se desarrollan y se evalúan las nuevas propuestas planteadas.
	\item \textbf{Extracción de resultados.} Los resultados de las distintas evaluaciones deben ser recopilados y guardados adecuadamente, a fin de poder reproducirlos más adelante en formato de gráficas o tablas que ayuden a defender las conclusiones del estudio.
\end{enumerate}

A continuación se realizará un recorrido fase por fase explicando con más precisión los distintos conceptos que se han de tener en cuenta y los factores que han propiciado decantarse por una alternativa u otra en cada caso.

\section{Elección del conjunto de datos}

Cuando se trata de evaluar un sistema de recomendación de películas, existe un conjunto de datos por excelencia que sobresale con respecto al resto: se trata de la base de datos de \textbf{MovieLens} \cite{movielens} \cite{movielens-paper}. MovieLens es un recomendador de películas utilizado en investigación por la Universidad de Minnesota, y cuenta con un contrastado bagaje ya que lleva apareciendo en publicaciones desde prácticamente los inicios de los sistemas de recomendación. Sin ir más lejos, el primer sistema de recomendación a grupos, PolyLens \cite{polylens}, se basaba en esta base de datos.

Al ser mantenido por un grupo investigador, MovieLens ofrece una amplia variedad de bases de datos con las que trabajar, más pequeñas o más grandes, y desde las más antiguas datadas de 1998 hasta las más nuevas, que se actualizan con el tiempo. De acuerdo a la información extraída de su página web, las bases de datos más antiguas son estables, pero las más nuevas son más recomendables para trabajos universitarios e investigaciones.

En base al contrastado uso de MovieLens en labores de investigación, se ha decidido escogerlo como conjunto de datos sobre el que realizar el experimento. En particular, se ha trabajado con la base de datos \textit{100K}, que cuenta con 100.836 valoraciones aplicadas sobre 9.742 películas por 610 usuarios. Estas valoraciones se obtuvieron entre marzo de 1996 y septiembre de 2018, de cuando data el conjunto en el momento de la documentación de este trabajo. El contenido descargable se compone de 4 archivos de extensión \textit{.csv}, de los cuales para este estudio solo se han requerido \textit{ratings.csv} y \textit{movies.csv}, que contienen la información acerca de las películas (nombre, año) y de las valoraciones que los usuarios han dado a qué películas.

Para el estudio se requiere que exista un conjunto de entrenamiento y uno de validación, que será con respecto al que se realizarán las recomendaciones y que deberán contrastarse con los resultados reales. Para realizar la división, se ha hecho una partición 80-20, realizando una permutación aleatoria de las 9.742 películas y asignando el 80\% al conjunto de entrenamiento (7.794 películas) y el 20\% restante al conjunto de validación (1.948 películas).

\section{Creación de grupos}

Para poder evaluar un sistema de recomendación a grupos se necesita agrupar a los usuarios de los que se dispone en distintas fracciones, cada una de las cuales será recomendada por el sistema a la hora de la experimentación. Para este trabajo se han determinado dos condiciones que deben cumplir todos los grupos que han de ser evaluados: la primera es que deben tener un tamaño fijo y estipulado desde el inicio; la segunda es que un usuario pertenecerá únicamente a uno de los grupos -- no puede haber un usuario que aparezca en dos grupos ni puede haber un usuario que no pertenezca a ninguno.

Pueden seguirse distintas estrategias a la hora de definir las fronteras entre miembros. De Campos et al. \cite{umuai} planteaban la posibilidad de formar grupos agrupando a usuarios que fuesen considerados \textit{``buddies''} o \textit{colegas}. La idea detrás de su planteamiento consistía en únicamente formar grupos con aquellos usuarios que hubieran visto al menos una película en común, pero para un número determinado previamente, igual que en este experimento.

La experimentación de esta publicación, sin embargo, se realiza \textbf{creando grupos para cada película}, ya que su método predice la valoración que un grupo le va a dar a una película en concreto, en lugar de devolver un ranking. Por este motivo no se puede aplicar a este trabajo la estrategia de creación de grupos de la misma manera exacta.

De forma empírica se ha comprobado que siguiendo este criterio para la base de datos que se ha utilizado en el trabajo aparecen pocos grupos. De tal forma, se quedan una gran cantidad de usuarios sin ser agrupados, resultando un experimento poco convincente. Sin embargo, como la idea de los grupos de colegas resulta bastante interesante, se ha decidido hacer una adaptación para este estudio. Se ha modificado la definición de ``colegas'' a ``usuarios con mayor correlación'', y se ha hecho uso del coeficiente de Pearson para obtener esta información.

El coeficiente de Pearson es un parámetro que determina cómo de correladas están dos variables, que en este caso van a ser los usuarios. Las ``características'' de estos usuarios serían las películas y las valoraciones que le han dado a cada una de ellas, pero para calcular el coeficiente entre dos personas solo se han de tener en cuenta aquellas películas que ambos hayan visto. No debe influir en la correlación entre dos usuarios que uno haya visto muchas más películas que el otro, sino cómo de parecidas son las valoraciones que han dado a películas en común que han visto. Calculando la correlación de cada usuario con respecto a los demás se puede construir la matriz de correlación de Pearson, que contiene este cálculo para cada fila y para cada columna, situándose en ambos ejes todos los usuarios del sistema. Por la propiedad simétrica de la correlación, el resultado es por supuesto una matriz simétrica, con la diagonal siendo siempre necesariamente 1 (un usuario es igual a sí mismo).

Es importante notar que la correlación debe calcularse siempre \textbf{utilizando el conjunto de entrenamiento}, y nunca usando el de validación. Esos datos deben limitarse a la evaluación de la predicción del experimento y no deben influir en ningún caso en la toma de decisiones del sistema de recomendación.

Una vez calculada esta matriz, se han generado los grupos de colegas utilizando esta correlación. Se ha fijado el tamaño del grupo a 5 usuarios, un número realista para unos amigos que se reúnen con frecuencia para ir al cine, y se ha seguido el siguiente criterio para agruparlos: se escoge al azar un usuario sin agrupar, mediante la matriz de correlación de Pearson se encuentran los 4 usuarios más correlados con él y entre todos forman un grupo y dejan de poder ser agrupados en otro. Esta es la interpretación que se le ha dado a los \textbf{grupos de colegas} para este estudio, y con este algoritmo nunca queda ningún miembro sin asignar (pues 610 es divisible entre 5).

Adicionalmente, para darle otro enfoque al experimento, se han generado también \textbf{grupos aleatorios} mediante muestreo sin reemplazo de 5 usuarios del conjunto inicial de 610 hasta que todos estén asignados a un grupo. Estas dos variantes serán las que se prueben en los sistemas de recomendación que se analizarán en el estudio.

\section{Guardado de información reutilizable de interés}

Hay una serie de valores relativamente costosos de obtener y que además son estáticos en el tiempo, con lo que no tiene sentido tener que calcularlos constantemente cada vez que se vaya a realizar una variante del experimento. Estos datos pueden generarse una única vez mediante un \textit{script} adicional y quedarse guardados en ficheros, de forma que cuando se ejecute el programa principal, el que contiene todo el grueso del experimento, se pueden simplemente leer los datos anteriormente calculados y no gastar tiempo ni recursos sin sentido en recalcular los mismos datos una y otra vez.

En particular el caso más importante es el de los grupos. La generación de grupos debe hacerse una única vez, y no debe influir en qué variantes se le apliquen a los archivos que contienen la evaluación. Por ello, se almacenan en dos archivos \textit{.csv}, uno para los grupos de colegas y otro para los grupos aleatorios.

Otro de los cálculos que pueden hacerse una única vez es el de la matriz de Pearson. Estos coeficientes se calculan únicamente en base a las películas del conjunto de entrenamiento, y por tanto no varían en el tiempo y no es necesario recalcularlos. Al formar todos ellos una matriz también se almacenan en formato \textit{.csv}.

Por supuesto las películas que pertenecen al conjunto de entrenamiento y al de validación deben ser guardadas también, a fin de no ser sobrescritas en ningún momento a partir de la creación de grupos, pues para ello ya influye qué películas pertenezcan a cada partición. Se guardan en formato \textit{.csv} ya que son en esencia una columna de identificadores.

Por último, se guardan las valoraciones de cada usuario para cada película, para tener esos datos más a mano y poder comprobar rápidamente, por ejemplo, cuál es la lista de películas visualizadas por un usuario en concreto. Esto es útil en varias situaciones a lo largo de la experimentación, por lo que se ha tomado la decisión de almacenar dicha información y recurrir a ella directamente cuando sea necesario. Se dividen en dos diccionarios, uno para las películas de entrenamiento y otro para las de validación, donde la clave es el identificador de usuario y el valor otro diccionario con pares \textit{\{película, valoración\}}. Al tratarse de una estructura más compleja, se han guardado en formato \textit{.txt} para poder ser leídos directamente con la misma estructura con la que se guardaron.

\section{Filtrado de películas evaluables por grupo}

Como se comentó en un apartado anterior, para poder valorar si un sistema de recomendación ha acertado colocando una película más arriba o más abajo para el ranking de un grupo, es necesario saber qué valoración le han dado realmente los miembros del grupo a esa película. Sin esta información no tiene sentido recomendar una película al grupo dado que no se tienen las herramientas adecuadas para probar la validez de dicha recomendación. Es por ello que es necesario filtrar qué películas se van a recomendar a cada grupo, escogiendo aquellas que cumplan los requisitos básicos para poder ser evaluadas.

El caso idóneo se da cuando todos los miembros del grupo han visto la película. En ese caso concreto, se tiene una fiabilidad máxima acerca de la valoración que le daría el grupo como conjunto. Pero como se está utilizando una base de datos realista, donde no todos los usuarios han visto todas las películas y existen una gran cantidad de ``vacíos'' por rellenar en la hipotética matriz constituida por usuarios y películas, hace falta ser un poco más flexible con esta condición. Es por ello que se ha decidido para este estudio que un grupo evalúe \textbf{todas aquellas películas que hayan visto al menos 3 componentes del mismo}.

A pesar de ser flexibles y de dar ese margen, hay grupos formados que no cuentan con ninguna película de dichas características, por lo que tales grupos son eliminados del experimento. Tras haber limpiado estos grupos siguen quedando suficientes como para considerar el proceso suficientemente interesante de ser evaluado.

Las películas a evaluar de cada grupo se calculan una única vez y también se almacenan, como se ha hecho con los datos del apartado anterior. En este caso esta información se tiene que almacenar en un archivo \textit{.txt} precisamente por tener una estructura un poco más compleja, siendo un diccionario de clave el identificador del grupo y de valor el vector de películas correspondiente.

\section{Diseño de la evaluación de los modelos}

En primer lugar, para poder evaluar de forma adecuada cualquier resultado obtenido de los sistemas de recomendación a estudiar, es necesario conocer cuál es el resultado que se espera. Para ello se ha de realizar un \textbf{ranking real} para cada uno de los grupos, tanto aleatorios como de colegas. En particular, como se busca contar con 4 estrategias de combinación distintas, se crearán \textbf{4 rankings reales}, cada uno de los cuales representará una estrategia tomada por el grupo.

Las 4 estrategias planteadas son las siguientes:

\begin{itemize}
	\item \textbf{Máximo}: la valoración dada por el grupo a una película será el máximo de las valoraciones individuales de sus miembros.
	\item \textbf{Mínimo}: la valoración dada por el grupo a una película será el mínimo de las valoraciones individuales de sus miembros.
	\item \textbf{Media}: la valoración dada por el grupo a una película será la media de las valoraciones individuales de sus miembros.
	\item \textbf{Mayoría}: la valoración dada por el grupo a una película será la valoración que más se repita entre sus miembros.
\end{itemize}

Para construir cada uno de los 4 rankings reales se utilizan las valoraciones obtenidas del conjunto de validación. Es de hecho el único momento en el que se puede hacer uso de este dato para la experimentación, a la hora de comparar las predicciones con la realidad. Si un usuario no ha visto una película, el cómputo se hace sin contarle a él, para no penalizar las métricas por falta de información.

Existen multitud de métricas de evaluación de carácter general, pero para un problema de las características de este estudio, teniendo que validar resultados ordenados como son los rankings, es necesario buscar la medida que más se ajuste a estas pretensiones.

En un primer momento se pensó en utilizar el \textbf{Mean Absolute Error}, calculando la diferencia de posiciones entre el ranking predicho y el ranking real. Sin embargo esto presenta un problema grave, y es que se les da la misma importancia a que haya dos películas mal situadas en la cabeza de la clasificación o en la cola. Siempre debe tener mucha más importancia acertar con las primeras recomendaciones, por lo que estos primeros puestos deberían tener sustancialmente más influencia en la validación del ranking que los últimos.

Para solventar este problema se ha utilizado finalmente como única métrica de evaluación la \textbf{Normalized Discounted Cummulative Gain (nDCG)}. Esta métrica computa la ``ganancia'' \textit{(gain)} del primer elemento de la lista y se va acumulando mientras continúa hacia posiciones inferiores. La ganancia calculada por cada elemento de la lista está graduada en función de lo alta que sea la posición en la que se encuentra, y por supuesto en función de lo acertada que sea con respecto al ranking real. Al tratarse además de la versión \textbf{normalizada} de la métrica, el resultado obtenido viene dado en el rango [0,1], siendo 1 un calco exacto del ranking real correspondiente

\section{Evaluación de los métodos del estudio}

Una vez diseñados el experimento y la evaluación del mismo, se pueden poner a prueba los distintos algoritmos del estudio.

Un punto en común que comparten todos los algoritmos es la necesidad de encontrar un método de predicción de valoraciones individuales para cada uno de los miembros del grupo. Para predecir esta valoración se ha utilizado la \textbf{fórmula del filtrado colaborativo} \cite{recuperacion-informacion}. Esta fórmula permite predecir la valoración de una película en función de los $k$ vecinos más cercanos al mismo, entendiendo por ``más cercanos'' aquellos que guardan una mayor correlación con el usuario (y no pertenecen a su mismo grupo). En esencia, se consulta para cada uno de los vecinos la valoración que han aportado a la película, y se multiplica por su coeficiente de correlación con el usuario. Si el usuario ha visto la película, además se añade el valor absoluto de su correlación a un parámetro de normalización. Al final del proceso, el acumulado de las valoraciones de los vecinos se divide entre el parámetro de normalización para dar como resultado la valoración predicha.

Este método se usa en todos los métodos propuestos a la hora de predecir inicialmente las valoraciones individuales que cada usuario le da a las películas a evaluar, pero también se utiliza en los algoritmos de referencia: el \textit{baseline} y el método del estado del arte. En la publicación del método del estado del arte \cite{pogrs} no se especifica que se deban predecir previamente las valoraciones de las películas a recomendar, simplemente se refiere a ``valoraciones provistas por los usuarios''. Para que el experimento sea justo con el resto de métodos, y entendiendo que no se debe trabajar nunca partiendo de la base de que se conocen las valoraciones del conjunto de validación, se ha decidido predecir estas valoraciones también para el método del estado del arte utilizando el filtrado colaborativo.

A continuación se explicará el funcionamiento del algoritmo \textit{baseline}. Un \textit{baseline} generalmente consiste en una fórmula relativamente sencilla que actúa como mínimo listón a superar por cualquier método que se proponga ser competente. Se trata de un algoritmo más bien básico, que en lugar de hacer propuestas innovadoras se limita a aplicar alguna fórmula cuya eficacia está más que probada. Si bien este método nunca estará entre los más destacados de la literatura, pues para ello se realizan investigaciones con métodos más complejos que abarcan muchos posibles frentes, se trata de un algoritmo contrastado que sirve de apoyo para estas investigaciones. Cualquier nueva propuesta que no supere los resultados de este algoritmo base puede considerarse poco eficaz.

El algoritmo que se ha utilizado como \textit{baseline} para el experimento ha sido también el del \textbf{filtrado colaborativo}, junto a cada una de las distintas estrategias de combinación que se han especificado en un apartado anterior. Mediante la fórmula del filtrado colaborativo se predicen las valoraciones individuales que cada usuario daría a cada película, tal y como se ha explicado en el funcionamiento de dicha fórmula. Una vez obtenidas las valoraciones, estas se combinan y se genera un ranking distinto para cada una de las estrategias mencionadas, obteniendo así rankings para la estrategia del máximo, del mínimo, de la media y de la mayoría.

Una vez que se han obtenido estos rankings se pueden comparar con los resultados reales. Dado que el algoritmo \textit{baseline} permite obtener un ranking distinto dependiendo de cada estrategia, se puede evaluar directamente con el ranking real correspondiente a la misma, dando lugar a una comparativa directa donde se evalúa principalmente la eficacia del filtrado colaborativo.

Como el resto de métodos del estudio (tanto la referencia del estado del arte como las nuevas propuestas) generan un único ranking en lugar de uno por cada estrategia de combinación, dicho ranking se somete a evaluación con respecto a cada uno de los rankings reales, dando una idea de cómo funcionaría cada método si la forma de combinar las valoraciones del grupo fuera cada una de las anteriormente planteadas.

\section{Extracción de resultados}

Como último paso de la experimentación, es imprescindible recopilar los resultados del mismo. A través de la evaluación, mediante la métrica \textbf{Normalized Discounted Cummulative Gain} se obtiene una puntuación que servirá para comparar la eficacia de los diferentes métodos. Además será interesante comparar sus puntuaciones no solamente en términos generales, sino dependiendo de la estrategia de combinación aplicada por el grupo.

Tras obtener las puntuaciones que cada ranking predicho obtiene con respecto a los rankings reales, esta información se guarda en un fichero, por cada método y por cada tipo de grupos (aleatorios o de colegas), quedando en total \textbf{12 archivos de resultados} (6 métodos a evaluar por 2 archivos de resultados por método). Cada archivo, a su vez, contiene dentro los valores nDCG resultantes de comparar la predicción generada por el método y el ranking real correspondiente a cada estrategia -- en total, se obtienen 4 valores por cada uno de dichos archivos.

La disposición escogida para el almacenamiento ordenado de estos datos permite disponer de ellos rápida y descriptivamente cuando se necesiten. De esta manera, construir las tablas o las gráficas necesarias para apoyar las conclusiones del estudio resulta una labor más directa.


